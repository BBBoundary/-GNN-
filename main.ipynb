{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "831da982",
   "metadata": {},
   "source": [
    "# 基于GNN的金融异常检测任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a3950",
   "metadata": {},
   "source": [
    "## 1. 实验介绍\n",
    "\n",
    "反欺诈是金融行业永恒的主题，在互联网金融信贷业务中，数字金融反欺诈技术已经得到广泛应用并取得良好效果，这其中包括了近几年迅速发展并在各个领域\n",
    "得到越来越广泛应用的图神经网络。本项目以互联网智能风控为背景，从用户相互关联和影响的视角，探索满足风控反欺诈领域需求的，可拓展、高效的图神经\n",
    "网络应用方案，从而帮助更好地识别欺诈用户。\n",
    "\n",
    "本项目主要关于实现预测模型(**不限于图神经网络**)，进行节点异常检测任务，并验证模型精度。而本项目基于的数据集[DGraph](https://dgraph.xinye.com/introduction)，[DGraph](https://dgraph.xinye.com/introduction)\n",
    "是大规模动态图数据集的集合，由真实金融场景中随着时间演变事件和标签构成。\n",
    "\n",
    "### 1.1 实验目的\n",
    "\n",
    "- 了解如何使用Pytorch进行神经网络训练\n",
    "- 了解如何使用Pytorch-geometric等图网络深度学习库进行简单图神经网络设计(推荐使用GAT, GraphSAGE模型)。\n",
    "- 了解如何利用MO平台进行模型性能评估。\n",
    "\n",
    "### 1.2 预备知识\n",
    "- 具备一定的深度学习理论知识，如卷积神经网络、损失函数、优化器，训练策略等。\n",
    "- 了解并熟悉Pytorch计算框架。\n",
    "- 学习Pytorch-geometric，请前往：https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "    \n",
    "### 1.3实验环境\n",
    "- numpy = 1.18.5  \n",
    "- pytorch = 1.4.0  \n",
    "- torch_geometric = 1.7.0  \n",
    "- torch_scatter = 2.0.3  \n",
    "- torch_sparse = 0.5.1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28041bc6",
   "metadata": {},
   "source": [
    "## 2. 实验内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af3383",
   "metadata": {},
   "source": [
    "### 2.1 数据集信息\n",
    "DGraph-Fin 是一个由数百万个节点和边组成的有向无边权的动态图。它代表了Finvolution Group用户之间的社交网络，其中一个节点对应一个Finvolution 用户，从一个用户到另一个用户的边表示**该用户将另一个用户视为紧急联系人**。\n",
    "下面是`位于dataset/DGraphFin目录`的DGraphFin数据集的描述:\n",
    "```\n",
    "x:  20维节点特征向量\n",
    "y:  节点对应标签，一共包含四类。其中类1代表欺诈用户而类0代表正常用户(实验中需要进行预测的两类标签)，类2和类3则是背景用户，即无需预测其标签。\n",
    "edge_index:  图数据边集,每条边的形式(id_a,id_b)，其中ids是x中的索引\n",
    "edge_type: 共11种类型的边\n",
    "edge_timestamp: 脱敏后的时间戳\n",
    "train_mask, valid_mask, test_mask: 训练集，验证集和测试集掩码\n",
    "```\n",
    "本预测任务为识别欺诈用户的节点预测任务,只需要将欺诈用户（Class 1）从正常用户（Class 0）中区分出来。需要注意的是，其中测试集中样本对应的label**均被标记为-100**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1df9a",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### 2.2 导入相关包\n",
    "\n",
    "导入相应模块，设置数据集路径、设备等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b893cb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SoftWare\\Anaconda3\\envs\\pytorch-cuda115\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import DGraphFin\n",
    "from utils.utils import prepare_folder\n",
    "from utils.evaluator import Evaluator\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "\n",
    "#设置gpu设备\n",
    "device = 0\n",
    "device = 'cpu' #f'cuda:{device}' if torch.cuda.is_available() else \n",
    "device = torch.device(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179bfe5b",
   "metadata": {},
   "source": [
    "### 2.3 数据处理\n",
    "\n",
    "在使用数据集训练网络前，首先需要对数据进行归一化等预处理，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d73bd7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "path='./datasets/632d74d4e2843a53167ee9a1-momodel/' #数据保存路径\n",
    "save_dir='./results/' #模型保存路径\n",
    "dataset_name='DGraph'\n",
    "dataset = DGraphFin(root=path, name=dataset_name, transform=T.ToSparseTensor(remove_edge_index=False))\n",
    "\n",
    "nlabels = dataset.num_classes\n",
    "if dataset_name in ['DGraph']:\n",
    "    nlabels = 2    #本实验中仅需预测类0和类1\n",
    "\n",
    "data = dataset[0]\n",
    "data.adj_t = data.adj_t.to_symmetric() #将有向图转化为无向图\n",
    "\n",
    "\n",
    "if dataset_name in ['DGraph']:\n",
    "    x = data.x\n",
    "    x = (x - x.mean(0)) / x.std(0)\n",
    "    data.x = x\n",
    "if data.y.dim() == 2:\n",
    "    data.y = data.y.squeeze(1)\n",
    "\n",
    "split_idx = {'train': data.train_mask, 'valid': data.valid_mask, 'test': data.test_mask}  #划分训练集，验证集\n",
    "\n",
    "train_idx = split_idx['train']\n",
    "valid_idx = split_idx['valid']\n",
    "test_idx = split_idx['test']\n",
    "\n",
    "result_dir = prepare_folder(dataset_name,'mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "cefd90af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4300999])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e525b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([857899])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32e9bbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(row=tensor([      0,       0,       1,  ..., 3700548, 3700548, 3700549]),\n",
       "             col=tensor([ 826823, 3145251, 1116391,  ..., 2110278, 2609004, 2570092]),\n",
       "             size=(3700550, 3700550), nnz=7994520, density=0.00%)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.adj_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70055b60",
   "metadata": {},
   "source": [
    "这里我们可以查看数据各部分维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d834762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3700550, 20], edge_attr=[4300999], y=[3700550], train_mask=[857899], valid_mask=[183862], test_mask=[183840], adj_t=[3700550, 3700550, nnz=7994520])\n",
      "torch.Size([3700550, 20])\n",
      "torch.Size([3700550])\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(data.x.shape)  #feature\n",
    "print(data.y.shape)  #label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661a55a",
   "metadata": {},
   "source": [
    "### 2.4 定义模型\n",
    "这里我们使用简单的多层感知机作为例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38aaa55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    图注意力层\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features  # 节点表示向量的输入特征维度\n",
    "        self.out_features = out_features  # 节点表示向量的输出特征维度\n",
    "        self.dropout = dropout  # dropout参数\n",
    "        self.alpha = alpha  # leakyrelu激活的参数\n",
    "        self.concat = concat  # 如果为true, 再进行elu激活\n",
    "\n",
    "        # 定义可训练参数，即论文中的W和a\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)  # xavier初始化\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)  # xavier初始化\n",
    "\n",
    "        # 定义leakyrelu激活函数\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, inp, adj):\n",
    "        \"\"\"\n",
    "        inp: input_fea [N, in_features]  in_features表示节点的输入特征向量元素个数\n",
    "        adj: 图的邻接矩阵 维度[N, N] 非零即一，数据结构基本知识\n",
    "        \"\"\"\n",
    "        h = torch.mm(inp, self.W)  # [N, out_features]\n",
    "        N = h.size()[0]  # N 图的节点数\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        # [N, N, 2*out_features]\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        # [N, N, 1] => [N, N] 图注意力的相关系数（未归一化）\n",
    "\n",
    "        zero_vec = -1e12 * torch.ones_like(e)  # 将没有连接的边置为负无穷\n",
    "        attention = torch.where(adj > 0, e, zero_vec)  # [N, N]\n",
    "        # 表示如果邻接矩阵元素大于0时，则两个节点有连接，该位置的注意力系数保留，\n",
    "        # 否则需要mask并置为非常小的值，原因是softmax的时候这个最小值会不考虑。\n",
    "        attention = F.softmax(attention, dim=1)  # softmax形状保持不变 [N, N]，得到归一化的注意力权重！\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)  # dropout，防止过拟合\n",
    "        h_prime = torch.matmul(attention, h)  # [N, N].[N, out_features] => [N, out_features]\n",
    "        # 得到由周围节点通过注意力权重进行更新的表示\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29776f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        \"\"\"Dense version of GAT\n",
    "        n_heads 表示有几个GAL层，最后进行拼接在一起，类似self-attention\n",
    "        从不同的子空间进行抽取特征。\n",
    "        \"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 定义multi-head的图注意力层\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in\n",
    "                           range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)  # 加入pytorch的Module模块\n",
    "        # 输出层，也通过图注意力层来实现，可实现分类、预测等功能\n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # dropout，防止过拟合\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)  # 将每个head得到的表示进行拼接\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # dropout，防止过拟合\n",
    "        x = F.elu(self.out_att(x, adj))  # 输出并激活\n",
    "        return F.log_softmax(x, dim=1)  # log_softmax速度变快，保持数值稳定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252b0ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-9.0000e+15,  3.6447e-01,  2.7290e-01],\n",
      "        [-5.0304e-01, -9.0000e+15, -9.0000e+15],\n",
      "        [-7.1735e-01, -9.0000e+15, -9.0000e+15]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5229, 0.4771],\n",
       "        [1.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wh = torch.randn(3,5) # 3个节点，每个节点5个特征\n",
    "A = torch.randn(3,3) # 注意力系数矩阵\n",
    "# 邻接矩阵\n",
    "adj = torch.tensor([[0,1,1],\n",
    "                    [1,0,0],\n",
    "                    [1,0,0]])\n",
    "zero_vec = -9e15*torch.ones_like(A)\n",
    "attention = torch.where(adj>0, A, zero_vec)\n",
    "print(attention)\n",
    "attention = F.softmax(attention, dim=1)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82ef024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3700550"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b47b86be",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\MasterDegree\\course\\人工智能\\work2\\main.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m H\u001b[39m=\u001b[39msplit_idx[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m H \u001b[39m=\u001b[39m H \u001b[39m-\u001b[39m H\u001b[39m.\u001b[39;49msum(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# # 特征求和\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m rowsum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(H\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m),dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "H = H - H.sum(1)\n",
    "# # 特征求和\n",
    "rowsum = np.array(H.sum(1),dtype='float32')\n",
    "print(rowsum.shape)\n",
    "# # 倒数\n",
    "# r_inv = np.power(rowsum,-1).flatten()\n",
    "# # 解决除0问题\n",
    "# r_inv[np.isinf(r_inv)] = 0.\n",
    "# # 转换为对角阵\n",
    "# r_mat_inv = np.diag(r_inv)\n",
    "# # 对角阵乘以H，得到标准化矩阵\n",
    "# H = r_mat_inv.dot(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d71f39d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([857899, 20])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b1222a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.0435)\n",
      "tensor(-18.7909)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(36917.3438)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H=data.x[split_idx['train']]\n",
    "print(H.min())\n",
    "rowsum = H.sum(1)\n",
    "print(rowsum.min())\n",
    "for i in range (H.shape[0]):\n",
    "    H[i] = H[i]/rowsum[i]\n",
    "H.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "619e4d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([183840])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_idx['test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "049e212e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(row=tensor([     0,      0,      1,  ..., 857897, 857898, 857898]),\n",
       "             col=tensor([ 100846,  697288, 1487371,  ..., 2056144, 1194573, 3252165]),\n",
       "             size=(857899, 3700550), nnz=2235182, density=0.00%)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.adj_t[split_idx['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4e32c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "mx = sp.csr_matrix(data.x[split_idx['train']])\n",
    "rowsum = np.array(mx.sum(1))\n",
    "r_inv = np.power(rowsum, -1).flatten()\n",
    "r_inv[np.isinf(r_inv)] = 0.\n",
    "r_mat_inv = sp.diags(r_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "933bc5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<857899x857899 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 857899 stored elements (1 diagonals) in DIAgonal format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_mat_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0093d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "import scipy.sparse as sp\n",
    "# it's neccessary to use sparse-matrix for nomalization\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize \"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt) # need sparse matrix !\n",
    "    print(type(mx))\n",
    "    mx.tocsc\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize \"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv) # need sparse matrix !\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b3efc5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(row=tensor([     0,      0,      1,  ..., 183838, 183839, 183839]),\n",
       "             col=tensor([1826595, 3339795, 1057328,  ..., 3281558, 1163259, 3163502]),\n",
       "             size=(183840, 3700550), nnz=479208, density=0.00%)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.adj_t[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25862566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a255c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#常规label转one-hot向量\n",
    "def encode_onehot(labels):            #用单位矩阵来构建onehot向量\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in        #单位矩阵\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2c2a67bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneh = encode_onehot(np.arange(10))\n",
    "print(oneh)\n",
    "torch.LongTensor(np.where(oneh)[1]).type_as(torch.LongTensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d20c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(oneh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f56780e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7697bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 8 \n",
    "dropout = 0.6\n",
    "nb_heads = 8 \n",
    "\n",
    "alpha = 0.2\n",
    "lr = 0.005\n",
    "weight_decay = 5e-4\n",
    "epochs = 10000\n",
    "patience = 100\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# 实例化模型 \n",
    "model = GAT(n_feat=data.x.shape[1], \n",
    "            n_hid=hidden, \n",
    "            n_class=nlabels,\n",
    "            dropout=dropout, \n",
    "            n_heads=nb_heads, \n",
    "            alpha=alpha)\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                       lr=lr, \n",
    "                       weight_decay=weight_decay)\n",
    "# if cuda:\n",
    "#     model.cuda()\n",
    "#     features = features.cuda()\n",
    "#     adj = adj.cuda()\n",
    "#     labels = labels.cuda()\n",
    "#     idx_train = idx_train.cuda()\n",
    "#     idx_val = idx_val.cuda()\n",
    "#     idx_test = idx_test.cuda()\n",
    "\n",
    "# features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c65ed438",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 12698792577800 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\MasterDegree\\course\\人工智能\\work2\\main.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data\u001b[39m.\u001b[39;49madj_t[train_idx]\u001b[39m.\u001b[39;49mto_dense()\n",
      "File \u001b[1;32md:\\SoftWare\\Anaconda3\\envs\\pytorch-cuda115\\lib\\site-packages\\torch_sparse\\tensor.py:484\u001b[0m, in \u001b[0;36mSparseTensor.to_dense\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    481\u001b[0m     mat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msizes(), dtype\u001b[39m=\u001b[39mvalue\u001b[39m.\u001b[39mdtype,\n\u001b[0;32m    482\u001b[0m                       device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice())\n\u001b[0;32m    483\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     mat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msizes(), dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice())\n\u001b[0;32m    486\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    487\u001b[0m     mat[row, col] \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 12698792577800 bytes."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6e1f7d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3700550x3700550 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 7994520 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj=data.adj_t.to_scipy()\n",
    "adj\n",
    "# zero_vec = -1e12 * torch.ones_like(e)  # 将没有连接的边置为负无穷\n",
    "# attention = torch.where(adj > 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "efeb7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = data.adj_t\n",
    "features = torch.FloatTensor(normalize_features(data.x))\n",
    "labels = data.y\n",
    "\n",
    "train_idx = torch.LongTensor(train_idx)\n",
    "valid_idx = torch.LongTensor(valid_idx)\n",
    "test_idx = torch.LongTensor(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8f75dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    # trian\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[train_idx], labels[train_idx])\n",
    "    acc_train = accuracy(output[train_idx], labels[train_idx])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    # eval\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_val = F.nll_loss(output[split_idx], labels[split_idx])\n",
    "    acc_val = accuracy(output[split_idx], labels[split_idx])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f0c3f387",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 438210249680000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\MasterDegree\\course\\人工智能\\work2\\main.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(\u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;32md:\\MasterDegree\\course\\人工智能\\work2\\main.ipynb Cell 37\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m output \u001b[39m=\u001b[39m model(features, adj)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss_train \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(output[train_idx], labels[train_idx])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m acc_train \u001b[39m=\u001b[39m accuracy(output[train_idx], labels[train_idx])\n",
      "File \u001b[1;32md:\\SoftWare\\Anaconda3\\envs\\pytorch-cuda115\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\MasterDegree\\course\\人工智能\\work2\\main.ipynb Cell 37\u001b[0m in \u001b[0;36mGAT.forward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, adj):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)  \u001b[39m# dropout，防止过拟合\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([att(x, adj) \u001b[39mfor\u001b[39;00m att \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattentions], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# 将每个head得到的表示进行拼接\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)  \u001b[39m# dropout，防止过拟合\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_att(x, adj))  \u001b[39m# 输出并激活\u001b[39;00m\n",
      "\u001b[1;32md:\\MasterDegree\\course\\人工智能\\work2\\main.ipynb Cell 37\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, adj):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)  \u001b[39m# dropout，防止过拟合\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([att(x, adj) \u001b[39mfor\u001b[39;00m att \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattentions], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# 将每个head得到的表示进行拼接\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)  \u001b[39m# dropout，防止过拟合\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_att(x, adj))  \u001b[39m# 输出并激活\u001b[39;00m\n",
      "File \u001b[1;32md:\\SoftWare\\Anaconda3\\envs\\pytorch-cuda115\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\MasterDegree\\course\\人工智能\\work2\\main.ipynb Cell 37\u001b[0m in \u001b[0;36mGraphAttentionLayer.forward\u001b[1;34m(self, inp, adj)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(inp, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW)  \u001b[39m# [N, out_features]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m N \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m]  \u001b[39m# N 图的节点数\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m a_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h\u001b[39m.\u001b[39;49mrepeat(\u001b[39m1\u001b[39;49m, N)\u001b[39m.\u001b[39mview(N \u001b[39m*\u001b[39m N, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), h\u001b[39m.\u001b[39mrepeat(N, \u001b[39m1\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mview(N, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# [N, N, 2*out_features]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MasterDegree/course/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/work2/main.ipynb#X44sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m e \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleakyrelu(torch\u001b[39m.\u001b[39mmatmul(a_input, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma)\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 438210249680000 bytes."
     ]
    }
   ],
   "source": [
    "train(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96789f",
   "metadata": {},
   "source": [
    "### 2.5 训练\n",
    "\n",
    "使用训练集中的节点用于训练模型，并使用验证集进行挑选模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa1003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    # trian\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[train_idx], labels[train_idx])\n",
    "    acc_train = accuracy(output[train_idx], labels[train_idx])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    # eval\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_val = F.nll_loss(output[valid_idx], labels[valid_idx])\n",
    "    acc_val = accuracy(output[valid_idx], labels[valid_idx])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()\n",
    "\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    \n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f68271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data, split_idx, evaluator):\n",
    "    # data.y is labels of shape (N, )\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        losses, eval_results = dict(), dict()\n",
    "        for key in ['train', 'valid']:\n",
    "            node_id = split_idx[key]\n",
    "            \n",
    "            out = model(data.x[node_id])\n",
    "            y_pred = out.exp()  # (N,num_classes)\n",
    "            \n",
    "            losses[key] = F.nll_loss(out, data.y[node_id]).item()\n",
    "            eval_results[key] = evaluator.eval(data.y[node_id], y_pred)[eval_metric]\n",
    "\n",
    "    return eval_results, losses, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "294cc9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2946\n",
      "Epoch: 10, Loss: 0.0876, Train: 63.245, Valid: 63.668 \n",
      "Epoch: 20, Loss: 0.0939, Train: 67.276, Valid: 67.213 \n",
      "Epoch: 30, Loss: 0.0751, Train: 69.646, Valid: 69.094 \n",
      "Epoch: 40, Loss: 0.0655, Train: 68.973, Valid: 68.822 \n",
      "Epoch: 50, Loss: 0.0654, Train: 69.272, Valid: 68.871 \n",
      "Epoch: 60, Loss: 0.0645, Train: 70.279, Valid: 69.782 \n",
      "Epoch: 70, Loss: 0.0645, Train: 70.677, Valid: 70.147 \n",
      "Epoch: 80, Loss: 0.0643, Train: 70.765, Valid: 70.201 \n",
      "Epoch: 90, Loss: 0.0642, Train: 70.967, Valid: 70.355 \n",
      "Epoch: 100, Loss: 0.0641, Train: 71.163, Valid: 70.517 \n",
      "Epoch: 110, Loss: 0.0640, Train: 71.315, Valid: 70.636 \n",
      "Epoch: 120, Loss: 0.0640, Train: 71.427, Valid: 70.724 \n",
      "Epoch: 130, Loss: 0.0639, Train: 71.547, Valid: 70.805 \n",
      "Epoch: 140, Loss: 0.0639, Train: 71.641, Valid: 70.881 \n",
      "Epoch: 150, Loss: 0.0638, Train: 71.730, Valid: 70.949 \n",
      "Epoch: 160, Loss: 0.0638, Train: 71.805, Valid: 71.004 \n",
      "Epoch: 170, Loss: 0.0638, Train: 71.872, Valid: 71.047 \n",
      "Epoch: 180, Loss: 0.0638, Train: 71.933, Valid: 71.086 \n",
      "Epoch: 190, Loss: 0.0637, Train: 71.989, Valid: 71.124 \n",
      "Epoch: 200, Loss: 0.0637, Train: 72.040, Valid: 71.160 \n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))  #模型总参数量\n",
    "\n",
    "model.reset_parameters()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=para_dict['lr'], weight_decay=para_dict['weight_decay'])\n",
    "best_valid = 0\n",
    "min_valid_loss = 1e8\n",
    "\n",
    "for epoch in range(1,epochs + 1):\n",
    "    loss = train(model, data, train_idx, optimizer)\n",
    "    eval_results, losses, out = test(model, data, split_idx, evaluator)\n",
    "    train_eval, valid_eval = eval_results['train'], eval_results['valid']\n",
    "    train_loss, valid_loss = losses['train'], losses['valid']\n",
    "\n",
    "    if valid_loss < min_valid_loss:\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_dir+'/model.pt') #将表现最好的模型保存\n",
    "\n",
    "    if epoch % log_steps == 0:\n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "              f'Loss: {loss:.4f}, '\n",
    "              f'Train: {100 * train_eval:.3f}, ' # 我们将AUC值乘上100，使其在0-100的区间内\n",
    "              f'Valid: {100 * valid_eval:.3f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812fc4d",
   "metadata": {
    "inputHidden": false
   },
   "source": [
    "### 2.6 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4405a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_dir+'/model.pt')) #载入验证集上表现最好的模型\n",
    "def predict(data,node_id):\n",
    "    \"\"\"\n",
    "    加载模型和模型预测\n",
    "    :param node_id: int, 需要进行预测节点的下标\n",
    "    :return: tensor, 类0以及类1的概率, torch.size[1,2]\n",
    "    \"\"\"\n",
    "    # -------------------------- 实现模型预测部分的代码 ---------------------------\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(data.x[node_id])\n",
    "        y_pred = out.exp()  # (N,num_classes)\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c61bbd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9965, 0.0035])\n",
      "节点 0 预测对应的标签为:0, 为正常用户。\n",
      "tensor([0.9733, 0.0267])\n",
      "节点 1 预测对应的标签为:0, 为正常用户。\n"
     ]
    }
   ],
   "source": [
    "dic={0:\"正常用户\",1:\"欺诈用户\"}\n",
    "node_idx = 0\n",
    "y_pred = predict(data, node_idx)\n",
    "print(y_pred)\n",
    "print(f'节点 {node_idx} 预测对应的标签为:{torch.argmax(y_pred)}, 为{dic[torch.argmax(y_pred).item()]}。')\n",
    "\n",
    "node_idx = 1\n",
    "y_pred = predict(data, node_idx)\n",
    "print(y_pred)\n",
    "print(f'节点 {node_idx} 预测对应的标签为:{torch.argmax(y_pred)}, 为{dic[torch.argmax(y_pred).item()]}。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40ee1b",
   "metadata": {},
   "source": [
    "## 3. 作业评分\n",
    "\n",
    "**作业要求**：    \n",
    "                         \n",
    "1. 请加载你认为训练最佳的模型（不限于图神经网络)\n",
    "2. 提交的作业包括【程序报告.pdf】和代码文件。\n",
    "\n",
    "**注意：**\n",
    "          \n",
    "1. 在训练模型等过程中如果需要**保存数据、模型**等请写到 **results** 文件夹，如果采用 [离线任务](https://momodel.cn/docs/#/zh-cn/%E5%9C%A8GPU%E6%88%96CPU%E8%B5%84%E6%BA%90%E4%B8%8A%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B) 请务必将模型保存在 **results** 文件夹下。\n",
    "2. 训练出自己最好的模型后，先按照下列 cell 操作方式实现 NoteBook 加载模型测试；请测试通过在进行【系统测试】。\n",
    "3. 点击左侧栏`提交作业`后点击`生成文件`则只需勾选 `predict()` 函数的cell，即【**模型预测代码答题区域**】的 cell。\n",
    "4. 请导入必要的包和第三方库 (包括此文件中曾经导入过的)。\n",
    "5. 请加载你认为训练最佳的模型，即请按要求填写**模型路径**。\n",
    "6. `predict()`函数的输入和输出请不要改动。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b236f",
   "metadata": {},
   "source": [
    "===========================================  **模型预测代码答题区域**  =========================================== \n",
    "\n",
    "在下方的代码块中编写 **模型预测** 部分的代码，请勿在别的位置作答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da2644cf",
   "metadata": {
    "select": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "## 生成 main.py 时请勾选此 cell\n",
    "from utils import DGraphFin\n",
    "from utils.evaluator import Evaluator\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "nlabels = 2    #本实验中仅需预测类0和类1\n",
    "init_params = {\n",
    "    'num_layers': 2,\n",
    "    'hidden_channels': 128,\n",
    "    'dropout': 0.0,\n",
    "    'batchnorm': False\n",
    "}\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self\n",
    "                 , in_channels\n",
    "                 , hidden_channels\n",
    "                 , out_channels\n",
    "                 , num_layers\n",
    "                 , dropout\n",
    "                 , batchnorm=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        self.batchnorm = batchnorm\n",
    "        if self.batchnorm:\n",
    "            self.bns = torch.nn.ModuleList()\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "            if self.batchnorm:\n",
    "                self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "        if self.batchnorm:\n",
    "            for bn in self.bns:\n",
    "                bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x):    \n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            if self.batchnorm:\n",
    "                x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "def predict(data,node_id):\n",
    "    \"\"\"\n",
    "    加载模型和模型预测\n",
    "    :param node_id: int, 需要进行预测节点的下标\n",
    "    :return: tensor, 类0以及类1的概率, torch.size[1,2]\n",
    "    \"\"\"\n",
    "    model = MLP(in_channels=data.x.size(-1), out_channels=nlabels,**init_params)\n",
    "    # 这里可以加载你的模型\n",
    "    model.load_state_dict(torch.load('./results/model.pt'))\n",
    "    # 模型预测时，测试数据已经进行了归一化处理\n",
    "    # -------------------------- 实现模型预测部分的代码 ---------------------------\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(data.x[node_id])\n",
    "        y_pred = out.exp()  # (N,num_classes)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd951e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers hidden_channels dropout batchnorm\n"
     ]
    }
   ],
   "source": [
    "init_param = {\n",
    "    'num_layers': 2,\n",
    "    'hidden_channels': 128,\n",
    "    'dropout': 0.0,\n",
    "    'batchnorm': False\n",
    "}\n",
    "print(*init_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f0f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch-cuda115')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2474686dee597e0645b63f925a6dfee25e0cbae1b085132c8ee12cc9231f3570"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
